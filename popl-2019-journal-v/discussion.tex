% \begin{itemize}
% \item explain how fully abstract overlay semantics could form one pass of a verified secure compiler.
% \item Sharing stack references accross component boundaries is supported
% \item Other notions of well-bracketedness (specifically one would be to allow different stacks)
% \end{itemize}
\subsection{Full Abstraction}
% - Full abstraction proofs difficult
Our formulation of WBCF and LSE using a fully abstract overlay semantics has an important advantage with respect to others.
Imagine that you are implementing a fully abstract compiler for a high-level language, i.e.\ a secure compiler that enforces high-level abstractions when interacting with untrusted target-language components.
Such a compiler would need to perform many things and enforce other high-level properties than just WBCF and LSE.

If such a compiler uses the \stktokens{} calling convention, then the security proof should not have to reprove security of \stktokens{}.
Ideally, it should just combine security proofs for the compiler's other functionality with our results about \stktokens{}.
We point out that our formulation enables such reuse.
Specifically, the compiler could be factored into a part that targets \srccm{}, followed by our embedding into \trgcm{}.
If the authors of the secure compiler can prove full abstraction of the first part (relying on WBCF and LSE in \srccm{}) and they can also prove that this first part generates well-formed and reasonable components, then full abstraction of the whole compiler follows by our result and transitivity of fully abstract compilation.
Perhaps other reusable components of secure compilers could be formulated similarly using some form of fully abstract overlay semantics, to obtain similar reusability of their security proofs.

% When creating fully-abstract compilers between low-level machines, it is a big challenge to work with the exposed addresses.
% In particular, if the compilation changes the code size of a block of code, then it may be observable and prevent full-abstraction from being proven.
% We would argue that when a compilation reaches a phase where addresses are exposed, then the compilation should no longer change the code.
% This does, however, pose the challenge that there might quite a few abstractions in difference between the language where addresses are hidden to a machine where they are not.
% We propose that this challenge is solved by implementing these abstractions in a number of overlay semantics.
% By implementing them one by one, one can deal with one abstraction at a time reducing the complexity of each necessary full-abstraction proof.


% - Need to compile to a machine with enforcement mechanisms - capability machine an option
% - Full abstraction proofs modular, so other full abstraction proofs could target \srccm{} and thus have more abstractions to work with than if \trgcm{} was the target.

%\subsection{Sharing the stack}
% ?

\begin{jversion}
  A compiler is secure when it enforces the properties of high-level languages which begs the question what properties should we enforce.
  When it comes to fully-abstract compilation, then the answer is that all the properties of the high-level language should be enforced, so the real question is what high-level language we would like.
  \stktokens{} ensures a standard call-return control-flow, but if we want a different kind of control-flow, for instance call/CC, then we need to come up with a different enforcement scheme.
  Further, many high-level languages have exceptions which is yet another form of control-flow which is also not supported by \stktokens{}.
  This goes to show how we must consider what high-level language we want in order to answer the question of what properties we must enforce to get a fully-abstract compiler.
  % Lau: Say something about address hiding?

  We conjecture that \stktokens{} can support a limited form of continuations and exceptions.
  Specifically, the continuations and exceptions would be passed to the callee as sealed code capabilities.
  In order for the continuations and exceptions to work with the stack, they would have to be sealed with the return seals.
  \DDin{I'm not convinced that these limitations cannot be avoided. Perhaps we should keep this discussion more tentative?}
  This requirement puts two strong limitations on continuations and exceptions.
  First of all, they can be used by the immediate callee, but they cannot be used in subsequent calls made by the callee (assuming they use \stktokens{}).
  The problem is that the callee would have to take up part of the stack for their local stack frame.
  This would break up the return token that should be used with the continuation (or exception).
  If the callee really needs to pass on the continuation, then they would have to construct a wrapper that unravels the stack and restores the necessary return token.
  Second, a call that passes continuations or exceptions in this way has to pass the same continuations and exceptions in every call.
  The problem is that just like the return code capability, the continuation and exception capabilities are non-linear.
  This means that a callee can store them for subsequent calls preventing us from using different continuations and exceptions for different calls.

  \dominique{Also: should we mention the limited additional control flow we seem to get if we omit the stack base check (where  adversaries can use a kind of non-preemptive multi-threading)?}

\begin{thesisonly}
  % Say something about horizontal vs vertical composition.
  Fully-abstract compilers compose vertically which means that if we defined a compiler phase that targets \srccm{}, then we could use the full-abstraction theorem immediately.
  However, fully-abstract compilers do not extend horizontally in the sense that if we add a new feature in the target and source language, then the full-abstraction theorem does not follow immediately.
  Intuitively, this is the case because the new feature may have an unforeseen interaction with existing features that breaks a desired property.
  In order to enforce more language properties on capability machines, it is reasonable to assume that new capability primitives may be necessary.
  \stktokens{}, for instance, requires linear capabilities, and \citet{skorstengaard_reasoning_2017} uses local capabilities to enforce WBCF and LSE.
  In order to implement address-hiding (i.e.\ have references that do not expose the address they actually point to), it is reasonable to believe that one would need a new capability primitive.
  After all, the capabilities we use here do not hide the addresses in any meaningful way, so a new ``address hiding'' capability would be necessary to enforce this property.
  If we extended \trgcm{} and \srccm{} with such new capabilities, then we would have to redo the entire full-abstraction proof.
\end{thesisonly}

  % new paragraph: conditional full-abstraction
  \dominique{This paragraph should perhaps also point out the link to the dynamic compromise paper?}
  Our full-abstraction theorem, Theorem~\ref{thm:full-abstraction}, is not pure full-abstraction as it requires the components to be reasonable and well-formed.
  In other words, if we were to define a compiler phase that targets \srccm{}, then we would also have to show that every program it generates is well-formed and reasonable in order to use the full-abstraction result.
  Without the reasonability constraint, \stktokens{} would have to enforce reasonability instead.
  That is, \stktokens{} would have to dynamically ensure that no return seals or means to get return seals are passed in calls.
  Essentially, such checks would protect the trusted code against itself which shouldn't be necessary in the first place.
  Instead, the compiler should generate reasonable code that never exhibits the unreasonable behaviour.
  This should be done in a compiler phase where more information about the original program is available, e.g.\ the compilation phase that commits to the low-level translation.
  Similarly for the syntactic constraints given by well-formedness.
  The compiler should make sure to generate code that satisfies the well-formedness judgement, so it can be executed by the machine.

  % TODO new paragraph: Machine checked proof. MANY details, any future work should be machine checked?
  One challenge in full-abstraction proofs is to relate the translation of program to the program it was translated from.
  Such a relation is often expressed as a back-translation  \citep{devriese_modular_2017}, i.e.\ a translation from the target language to the source language.
  When we use an overlay semantics, the back-translation becomes trivial because the source and target language are syntactically the same, so the identity can be used as the back-translation.
  If we have native call and return instructions in the source language, then the source language would be different from the target language, and we would have to use a non-trivial back-translation.
  Specifically, the back-translation would need to distinguish sequences of instructions that is the translation of a call from sequences of instructions that just look like a call.
  With overlay semantics, this is not a concern because everything that looks like a call is interpreted as a call.
\end{jversion}

\subsection{Practical Applicability}
We believe there are good arguments for practical applicability of \stktokens{}.
The strong security guarantees are proven in a way that is reusable as part of a bigger proof of compiler security.
Its costs are
\begin{itemize}
\item a constant and limited amount of checks on every boundary crossing.
\item possibly a small memory overhead because stack frames must be of non-zero length
\end{itemize}
The main caveat is that we rely on the assumption that capability machines like CHERI can be extended with linear capabilities in an efficient way.

Although this assumption can only be discharged by demonstrating an actual implementation with efficiency measurements, the following notes are based on private discussions with people from the CHERI team as well as our own thoughts on the matter.
As we understand it, the main problems to solve for adding linear capabilities to a capability machine like CHERI are related to the move semantics for instructions like \texttt{move}, \texttt{store} and \texttt{load}.
Processor optimizations like pipelining and out-of-order execution rely on being able to accurately predict the registers and memory that an instruction will write to and read from.
Our instructions are a bit clumsy from this point-of-view because, for example, \texttt{move} or \texttt{store} will zero the source register resp. memory location if the value being written is linear.
A solution for this problem could be to add separate instructions for moving, storing and loading linear registers at the cost of additional opcode space.
Adding splice and split will also consume some opcode space.

Another problem is caused by the move semantics for \texttt{load} in the presence of multiple hardware threads.
In this setting, zeroing out the source memory location must happen atomically to avoid race conditions where two hardware threads end up reading the same linear capability to their registers.
This means that a \texttt{load} of a linear capability should behave atomically, similar to a primitive compare-and-swap instruction.
This is in principle not a problem except that atomic instructions are significantly slower than a regular \texttt{load} (on the order of 10x slower or more).
When using \stktokens{}, loads of linear capabilities happen only when a thread has stored its return data capability on the stack and loads it back from there after a return.
Because the stack is a region of memory with very high thread affinity (no other hardware thread should access it, in principle), and which is accessed quite often, well-engineered caching could perhaps reduce the high overhead of atomic loads of linear capabilities.
% If such memory could be (mostly) kept exclusively locked in a cache close to the processor, the overhead of atomic loads in \stktokens{} might be significantly less than \texttt{load}'s worst case.
The processor could perhaps also (be told to) rely on the fact that race conditions should be impossible for loads from linear capabilities (which should in principle be non-aliased) and just use a non-atomic load in that case.

\begin{jversion}
Programs that use a C-like calling convention often allow programs to pass a stack reference in a call either to provide a reference for a piece of memory to work on or to indicate where the result of the call should be placed.
\stktokens{} does not support this as it expects to get a stack pointer for the entire stack it handed out.
\dominique{Err... I don't see the problem.
  Passing stack references to callees should work fine, as long as the callee treats them linearly (i.e.\ doesn't try to copy them).
  }
However, if we have allocated part of the stack for a different purpose and handed out a reference for it, then the stack pointer cannot reference the same memory as the stack pointer is a linear capability.
\dominique{I don't understand this sentence.}
However, while it is not possible to allocate a reference for the stack, one can easily imagine that the stack could still be used to pass and return values.
\dominique{Or this one...}
\end{jversion}

\dominique{Perhaps worth mentioning the problem with adding support for linear values in a real C compiler: how to explain to the compiler that it should treat the linear values linearly and not use optimizations that will break this.}

%%% Local Variables:
%%% TeX-master: "paper"
%%% End: