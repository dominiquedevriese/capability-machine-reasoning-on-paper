With the technical results established, it's worth taking a step back and discuss our work from a more high-level perspective.
In the next sections, we specifically give some more thoughts on our use of full abstraction to express WBCF and LSE, and the practical usability of \stktokens{}.

% \begin{itemize}
% \item explain how fully abstract overlay semantics could form one pass of a verified secure compiler.
% \item Sharing stack references accross component boundaries is supported
% \item Other notions of well-bracketedness (specifically one would be to allow different stacks)
% \end{itemize}
\subsection{Full Abstraction}
% - Full abstraction proofs difficult
Our formulation of WBCF and LSE using a fully abstract overlay semantics has an important advantage.
Imagine that you are implementing a fully abstract compiler for a high-level language, i.e.\ a secure compiler that enforces high-level abstractions when interacting with untrusted target-language components.
Such a compiler would need to perform many things and enforce other high-level properties than just WBCF and LSE.

If such a compiler uses the \stktokens{} calling convention, then the security proof should not have to reprove security of \stktokens{}.
Ideally, it should just combine security proofs for the compiler's other functionality with our results about \stktokens{}.
We point out that our formulation enables such reuse.
Specifically, the compiler could be factored into a part that targets \srccm{}, followed by our embedding into \trgcm{}.
If the authors of the secure compiler can prove full abstraction of the first part (relying on WBCF and LSE in \srccm{}) and they can also prove that this first part generates well-formed and reasonable components, then full abstraction of the whole compiler follows by our result and transitivity of fully abstract compilation.
Perhaps other reusable components of secure compilers could be formulated similarly using some form of fully abstract overlay semantics, to obtain similar reusability of their security proofs.

% When creating fully-abstract compilers between low-level machines, it is a big challenge to work with the exposed addresses.
% In particular, if the compilation changes the code size of a block of code, then it may be observable and prevent full-abstraction from being proven.
% We would argue that when a compilation reaches a phase where addresses are exposed, then the compilation should no longer change the code.
% This does, however, pose the challenge that there might quite a few abstractions in difference between the language where addresses are hidden to a machine where they are not.
% We propose that this challenge is solved by implementing these abstractions in a number of overlay semantics.
% By implementing them one by one, one can deal with one abstraction at a time reducing the complexity of each necessary full-abstraction proof.


% - Need to compile to a machine with enforcement mechanisms - capability machine an option
% - Full abstraction proofs modular, so other full abstraction proofs could target \srccm{} and thus have more abstractions to work with than if \trgcm{} was the target.

%\subsection{Sharing the stack}
% ?

\begin{jversion}
  A compiler is secure when it enforces the properties of high-level languages which begs the question what properties we should enforce.
  When it comes to fully-abstract compilation, then the answer is that all the (observational) equivalences in the high-level language should be enforced, so the real question is what high-level language we would like.
  \stktokens{} ensures a standard call-return control-flow, but if we want a different kind of control-flow, for instance call/cc or C's \textit{longjmp}, then we need to come up with a different enforcement scheme.
  Further, many high-level languages have exceptions as another form of control-flow not (yet) supported by \stktokens{}.
  This goes to show how we must consider what high-level language we want in order to answer the question of what properties we must enforce to get a fully-abstract compiler.
  % Lau: Say something about address hiding?

  We have not investigated support for continuations and exceptions in \stktokens{} thoroughly but we expect such support could be added.
  For exceptions, one approach would let callers provide callees with an additional capability for exceptional returns.
  This second capability would be similar to the code part of the return capability pair and signed with the same seal.
  The callee would be able to invoke it to signal that an exception has been thrown after which the caller's code would handle the exception, either by executing an exception handler or by unwinding its stack frame and passing the exception on to its own caller.
  Essentially, this would mean every function would be made responsible for unwinding its own stack frame.
  Continuations are more complicated but could perhaps be treated using similar ideas.
  Alternatively, it might also be possible to have a piece of central trusted code that does the stack unravelling for all stack frames.
  To do this, the trusted code would need to receive a copy of all return seals from the linker.

  % Say something about full-abstractions proofs and feature extensions
  Full abstraction of a compiler is a property that takes into account the whole source and target languages.
  In other words, a full abstraction proof must consider all features of a language to make sure that the features don't interact in a way that breaks language abstractions.
  If we have a fully abstract compiler and add a feature to the source or target language, then the new compiler is not necessarily fully abstract anymore.
  Full abstraction would have to be proven for the new compiler to make sure that the new feature does not break existing abstractions in the language.
  Our proof of full-abstraction for \stktokens{} targets a simple capability machine that may not be able to enforce the high-level language abstractions we want, e.g.\ address hiding.
  In other words, the full-abstraction proof cannot be reused immediately.
  However, \stktokens{} is still a good candidate for the enforcement mechanism for well-bracketed control-flow and local-state encapsulation in a real fully abstract compiler.
  Generally speaking, it is worth investigating enforcement mechanisms for full abstraction in a simple setting that allows to quickly try out ideas and verify that the enforcement works.

  % new paragraph: conditional full-abstraction
  Our full-abstraction theorem, Theorem~\ref{thm:full-abstraction} only applies to components that are reasonable and well-formed.
  In other words, if we were to define a compiler phase that targets \srccm{}, then we would also have to show that every program it generates is well-formed and reasonable in order to use the full-abstraction result.
  Without the reasonability constraint, \stktokens{} would have to enforce reasonability instead.
  That is, \stktokens{} would have to dynamically ensure that no return seals or means to obtain them are passed in calls.
  Such checks would impose a performance overhead and are in principle unnecessary for correct code.
  We have instead chosen to rely on the compiler to generate reasonable code that never exhibits the unreasonable behaviour.
  This should be done in a compiler phase where more information about the original program is available, e.g.\ the compilation phase that commits to the low-level translation.
  Similarly for the syntactic constraints given by well-formedness.
  The compiler should make sure to generate code that satisfies the well-formedness judgement, so it can be executed by the machine.

  % TODO new paragraph: Machine checked proof. MANY details, any future work should be machine checked?
  One challenge in full-abstraction proofs is to construct source language contexts that emulate the behavior of an arbitrary target language context.
  This construction is known as a back-translation  \citep{devriese_modular_2017}, i.e.\ a translation from the target language to the source language.
  When we use an overlay semantics, the back-translation becomes trivial because the source and target language are syntactically the same, so the identity can be used as the back-translation.
  If we have native call and return instructions in the source language, then the source language would be different from the target language, and we would have to use a non-trivial back-translation.
  Specifically, the back-translation would need to distinguish sequences of instructions that are the translation of a call from sequences of instructions that just look like a call.
  With overlay semantics, this is not a concern because everything that looks like a call is interpreted as a call.
\end{jversion}

\subsection{Linear Capabilities in reality}
We believe there are good arguments for practical applicability of \stktokens{}.
The strong security guarantees are proven in a way that is reusable as part of a bigger proof of compiler security.
Its costs are
\begin{itemize}
\item a constant and limited amount of checks on every boundary crossing.
\item possibly a small memory overhead because stack frames must be of non-zero length
\end{itemize}
The main caveat is that we rely on the assumption that capability machines like CHERI can be extended with linear capabilities in an efficient way.

Although this assumption can only be discharged by demonstrating an actual implementation with efficiency measurements, the following notes are based on private discussions with people from the CHERI team as well as our own thoughts on the matter.
As we understand it, the main problems for adding linear capabilities to a capability machine like CHERI are related to the move semantics for instructions like \texttt{move}, \texttt{store} and \texttt{load}.
Processor optimizations like pipelining and out-of-order execution rely on being able to accurately predict the registers and memory that an instruction will write to and read from.
Our instructions are a bit clumsy from this point-of-view because, for example, \texttt{move} or \texttt{store} will zero the source register resp. memory location if the value being written is linear.
A solution for this problem could be to add separate instructions for moving, storing and loading linear registers at the cost of additional opcode space.
Adding splice and split will also consume some opcode space.

Another problem is caused by the move semantics for \texttt{load} in the presence of multiple hardware threads.
In this setting, zeroing out the source memory location must happen atomically to avoid race conditions where two hardware threads end up reading the same linear capability to their registers.
This means that a \texttt{load} of a linear capability should behave atomically, similar to a primitive compare-and-swap instruction.
This is in principle not a problem except that atomic instructions are significantly slower than a regular \texttt{load} (on the order of 10x slower or more).
When using \stktokens{}, loads of linear capabilities happen only when a thread has stored its return data capability on the stack and loads it back from there after a return.
Because the stack is a region of memory with very high thread affinity (no other hardware thread should access it, in principle), and which is accessed quite often, well-engineered caching could perhaps reduce the high overhead of atomic loads of linear capabilities.
% If such memory could be (mostly) kept exclusively locked in a cache close to the processor, the overhead of atomic loads in \stktokens{} might be significantly less than \texttt{load}'s worst case.
The processor could perhaps also (be told to) rely on the fact that race conditions should be impossible for loads from linear capabilities (which should be non-aliased) and just use a non-atomic load in that case.

\subsection{Support for source language idioms and features}

\begin{jversion}
  Our calling convention supports all source language features that can be expressed in \srccm{}.
  This includes relatively basic features such as optimized tail calls (see Section~\ref{subsec:overlay-semantics}), stack-allocated variable-size arrays (like with C's alloca).
  Some other idioms require a bit more thought, such as the control flow primitives we discussed above.
  
  Also interesting is the idiom of passing stack references in calls, as is common in programming languages with a C-like calling convention.
  \stktokens{} supports stack references but with a couple of caveats.
  First of all, the stack capability is linear, so all references to the stack have to be linear.
  This means that the callee has to treat references linearly.
  Next, like the stack capability, the stack references must be given back to the caller on return, so they can reconstruct their original stack capability (which allows them to return). 
  Finally, the encapsulated local stack frame should be a continuous piece of memory (because it has to be addressable by a single capability: the data part of the return capability pair).
  Because of this, stack-allocated objects for which references are passed to callees must be allocated at the top or bottom of the caller's stack frame.
  An escape analysis could be used to statically determine where to put allocations and, in principle, the allocations could be reordered dynamically before a call.
  In summary, support for passing stack references as arguments to callees could be added to \stktokens{}, but this would probably require some changes in the compiler and, more importantly, would require the callee to take special care when manipulating such references.
  We are unsure whether it is realistic to apply this approach for existing C code.
\end{jversion}

%%% Local Variables:
%%% TeX-master: "paper"
%%% End: