===========================================================================
                            CSF2017 Review #19A
---------------------------------------------------------------------------
Paper #19: Reasoning about a Capability Machine with Local Capabilities -
           Provably Safe Stack and Return Pointer Management (without OS
           Support)
---------------------------------------------------------------------------

                      Overall merit: B. OK paper, but I will not champion
                                        it
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

High-level languages often enforce basic isolation properties across program modules.  However, when linking with legacy code, these guarantees are usually voided.  It's possible, though, to define hardware primitives that can protect principled code against whatever an adversary has to throw at it.  That is, there can be function calls back and forth between trusted and untrusted code, with guarantees about how the untrusted code can't corrupt trusted state, for instance.  Capability machines are one such approach, which received a popularity boost recently from the DARPA CRASH program.  Pointers are stored with information on which regions of memory they convey access to, not just information on the specific addresses they happen to be pointing to.  The hardware enforces strict rules on how pointers may be created or modified.  With the right rules, machine code gets significant flexibility in crafting and enforcing isolation policies.

This paper formalizes one such capability machine, suggests "design patterns" like a calling convention for writing defensive code, and presents a formal framework for establishing isolation for concrete programs, based on a logical relation.  The neat thing about these theorems is that they quantify effectively over any code that an adversary might have installed.  For that adversary code, no additional check in the style of type systems, nor any additional code rewriting in the style of SFI, is needed.

                      ===== Comments for author =====

I like this paper for its careful analysis of how security may be achieved on a kind of hardware that is growing in popularity.  I have to admit that I personally find it more appealing to require that code installed on a system come with proofs of good behavior (with type systems as one lightweight option), so that more of the object-capability reasoning style could be applied.  However, it's hard to deny that plenty of legacy code, implemented in unsafe languages, is going to be around for the foreseeable future.  It's very satisfying to see strong guarantees limiting the damage such code can do.

My main reservation about the paper is that it depends on substantial experience with step-indexed Kripke logical relations, to appreciate the details of the formal framework.  I don't have that experience, so I didn't manage to form a sufficient intuition of the details.  I also think this kind of proof involves so much detail work that it shouldn't be trusted without mechanized proofs -- which don't seem to have been done here.  So, I'm enthusiastic about the goals of the paper, and the techniques it presents come across as basically reasonable, but I can't reach significant confidence in their soundness or generality.  (It doesn't help that the only examples discussed are assembly programs that fit in quarter-page figures.  Limitations might become apparent connecting to, say, a fully abstract compiler for a typed source language.)

One limitation whose importance I have a hard time gauging: local capabilities may only be kept in registers or stack.  We can't have arbitrary heap data structures full of different local capabilities.  Is that an important limitation?  Do we ever want to do efficient lookup of local capabilities from semantically meaningful keys?

"For simplicity, we assume that memory allocated through malloc cannot be freed."  Is this a show-stopper assumption to scale up to realistic code?  And is it essential that malloc is a built-in instruction rather than a library function?

An instruction to clear a range of memory can simplify implementing your security scheme, but there can still be a fundamental cost to zeroing out many memory addresses.  We at least expect that realistic processors will execute clear-memory instructions with cycle counts proportional to the sizes of regions.  I'm thinking of the regions as corresponding to function local stack frames, which will tend to be pretty small, but empirical evaluation of the costs would be helpful.

How does all this adapt to the multicore setting?  Does a zero-out-range instruction appear to run atomically?  What about context switches in the midst of other standard code snippets you insert?

At one point in the paper, there is a "throw-away" remark that stack-smashing attacks are straightforward to block.  I don't see how, if each function receives a pointer conveying access to its full stack area.  Such pointers must be carefully restricted into subpointers for particular local variables, like arrays.  Wouldn't it be easy to forget to restrict or to do it incorrectly?  I suppose the main arguments of this paper are about protecting *well-written* programs from attackers, but then don't well-written programs just avoid buffer overflows statically?

I'm stuck on type-level details of the future-worlds relations in Section 4.1.1.  Temporary and permanent regions are expressed as 4-tuples, but the inference rules for future worlds operate on 5-tuples.  Why is that?

Can you give an intuition for why the logical relations should be step-indexed?  The restricted nature of local capabilities seems to suggest that they won't be involved in the kinds of self-reference that motivated the invention of step indexing.  Is it the global references that force step indexing?  In general, can you give an intuition for why simpler proof techniques wouldn't suffice?  (It's hard not to suspect a bit of overkill when the only example programs are as short as they are.)

Figure 3: I think the `load` case is missing *updatePc*.

In Figure 9, is the beginning of `g1` right, where the 3rd line seems to overwrite the program counter, effectively jumping elsewhere without saving a new return pointer?  Should the operand order be reversed?

===========================================================================
                            CSF2017 Review #19B
---------------------------------------------------------------------------
Paper #19: Reasoning about a Capability Machine with Local Capabilities -
           Provably Safe Stack and Return Pointer Management (without OS
           Support)
---------------------------------------------------------------------------

                      Overall merit: C. Weak paper, though I will not fight
                                        strongly against it
                 Reviewer expertise: 4. Expert

                         ===== Paper summary =====

This paper presents a formal semantics for a simplified capability
machine based on recent work on the CHERI processor. Capabilities were
introduced to support encapsulation and enforce control-flow
correctness, but these properties have never been formally
proved. This paper applies the logical-relation-based techniques to
prove some form of the encapsulation property over the formalized
abstract machine. It has also introduced the notion of local
capability which supports temporary transfer of authority during the
duration of a function call.  The paper presents a few examples to
show that the formal capability machine can indeed be used to deduce
the desirable properties on concrete assembly code.

                      ===== Comments for author =====

Formal reasoning about capability machines is a very important
problem.  There have been folklores regarding the superiority of
capabilities in the mainstream system community. Capabilities implemented
and enforced using efficient hardware (as demonstrated in recent work on
CHERI) can provide the protection support without requiring operating
system mechanisms (e.g., virtual memory). Many people
believe that capabilities would provide the sorts of encapsulation that we
would need in realistic software and hardware systems. Unfortunately,
none of these folklores have been formally specified and
verified. This paper has the potential of making an important advance
toward this important problem.

The paper, as written now, still contains several major
problems. First, the paper is not well motivated; it is unclear to
most readers why capabilities machine is a good idea at all. Second,
the formal machine is still very complex and extremely difficult to
follow (even after reading through the 82-page extended technical
report). Third, the formalization itself is also problematic; it is
unclear that the main theorem proved (Theorem 2) provides the types of
encapsulation property we actually need.

Here are my more detailed comments.

The introduction or Section 2 need to provide a good explanation of
the benefits of the capability-based approach. Even if capabilities
are desirable, it is unclear why the adversary code must use the same
runtime stack as the trusted code. The second contribution in the
Intro states that the capability machine "does not use separate
per-component stacks but a single, fairly standard, contiguous stack",
why is this a good idea?  It seems using separate stacks for different
components will greatly simplify the formalism.

In any case, the paper would benefit from a high-level overview of the
capability-based runtime architecture. Will there be any operating
system running on the machine?  How many different types of runtime
components do we see on the machine?  What are cross-component calls
like?  Are these supposed to run at the user level or within the OS
kernel? Is there concurrency? How will kernel threads or user processes
fit into the overall picture? What benefits will capabilities bring over
the existing non-capability-based hardware?

Section 2 gives a good formalization of a simplified capability
machine, but unfortunately, it does not cover the most important
aspects of this machine, that is, the formal semantics of those
instructions (or macros) related function calls and returns (e.g.,
call, scall, push, pop). Section 3 does give some informal explanations,
but they are very difficult to follow. After reading the TR, I realize
that most of these function call and return instructions are quite
complex macros, but even in the TR, they are not fully spelled out.

One way to control the complexity of these macros is to probably introduce
a more abstract notion of "stacks" and treat these function call/return macros
as actual built-in instructions in Figure 3. This way, we can see clearly
how function calls/returns would work under various settings described 
in Section 3.

As written now, because the semantics of scall and call are unclear,
it is quite difficult o understand the examples shown in Section 5.
It is also difficult to see the connection of Theorem 2 and the
high-level control-flow correctness and encapsulation properties
mentioned in the Introduction.

Minor:

Figure 3, the semantics for "load" should also have the "updatePc" wrapper.
In the conditions for the semantics for jmp,  the entry permission "e"
is almost identical to the end address (e); probably one of them should be
renamed.

===========================================================================
                            CSF2017 Review #19C
---------------------------------------------------------------------------
Paper #19: Reasoning about a Capability Machine with Local Capabilities -
           Provably Safe Stack and Return Pointer Management (without OS
           Support)
---------------------------------------------------------------------------

                      Overall merit: B. OK paper, but I will not champion
                                        it
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

This paper considers the problem of how to design a capability machine to ensure not only capability safety but well bracketed call/return semantics, using only hardware enforcement. The authors propose a design that combines elements of the old M-Machine (its enter capabilities) with some from the more recent CHERI (a restricted form of its local capabilities), and show how to enforce safe interaction with (potentially) adversarial code, defining a calling convention. They formalise the capability-safety of their machine by following the recent work of Devriese in the style of a sophisticated logical relation. They show by a few small examples how one can use it to reason formally about properties like safe encapsulation of local state, well-bracketed call return etc.

                      ===== Comments for author =====

The paper summarises an impressive body of work. Its main contribution is the capability machine design and calling convention, and these are well thought out, clearly explained and insightful.

Where the paper is weakest is in the explanation of the logical relation that captures the capability safety of the machine, and in explaining how this relates to reasoning about the safety of the calling convention (e.g. scall and the other macros). The authors have a very large amount of material to summarise in a relatively small amount of space, and the logical machinery is highly sophisticated and builds on years of advanced work in this space. So this was always going to be a non-trivial task.

Nonetheless, I think the presentation could be improved particularly in Section 4.1 and to a slightly lesser extent in Section 4.2. S4.1 suffers from the problem of moving straight from a prose description to Theorem 1, without ever linking the various pieces of notation appearing in Theorem 4.1 tot he preceding prose. This creates a major roadblock for the reader. 

This section could be significantly improved by giving the intuition for the role played by the public and private transitions (future worlds), explaining that the latter captures a more fine nuanced notion of how the world is allowed to evolve due to local capabilities. Some of this explanation appears later in Section 6 when discussing how the use of Dreyer et al.'s public/private future worlds differs here; however the general intuitions should be given in Section 4.1.

Almost none of the symbols appearing on page 7 are explained. The reader is left to figure out for themselves that phi is the public future state relation, phi_priv the private one, and the role these play, and to guess at what UPred might denote, amongst many other things. Ideally, each of these symbols would appear in the preceding explanatory text so that the reader can at least map the preceding concepts to what symbols denote each in the formalisation. Otherwise, the formulae may as well just be there for the benefit of only those steeped in logical relations, who will invariably refer to the extended version anyway.

Section 5 would also be improved by explaining better the role played by the logical relation in carrying out this reasoning. One can wade through the extended version to work it out for themselves, but shouldn't have to. A quick explanation of how, in general, one uses the preceding material to reason about programs would be highly beneficial.

Without these further explanations, much of the value of the formalisation is lost on the wider audience, and runs the risk of this part of the paper being seen as simply another instance of Devriese et al. EuroS&P 2016 in the context of a capability machine rather than a lambda calculus.


As an aside, to the above, the system wide limitation in the calling convention that local capabilities be used only for the single stack seems suboptimal. Why not adopt a model in which local capabilities also denote a "region" or "domain" to which they belong, of which there can be an arbitrary number, and stipulate that the adversary must have local capabilities only to their own distinct domain. One expects that a more general calling convention would emerge (necessarily complicating the logical relation and its existing notion of regions) that would e.g. allow the adversary to have their own stack if they wished (desirable if it wishes to ensure it won't run out of stack space) and allowing a more expressive style of programming. I can't say I was convinced by the footnote claiming that an adversary wanting to give temporary access to a callback that doesn't live on the single stack is not essential. What evidence is there for this claim? 

In general, the proposed approach is couched in terms of defending against an adversary (who is bad), who might be trying to attack "us" (who are necessarily good). In practice, capability systems are for programming in the presence of mutual distrust. In this case, the adversary wants to know they have enough stack space, might want to provide their own etc., and so one can't help but feel that the presented model is limited.

===========================================================================
  AR1 Response by Dominique Devriese <dominique.devriese@cs.kuleuven.be>
---------------------------------------------------------------------------
We thank the reviewers for thoroughly reading the paper. We will take
all comments into consideration for the final version of the paper.

Several reviewers commented on the complexity of the techniques we
used.  We understand the reservation and we are thankful for the
concrete suggestions of what we can try to explain better (which we
will definitely attempt to implement).  However, we would also like to
point out that from our point of view, the techniques we used are very
natural here: for a lambda calculus with similar features, they would
be considered reasonably standard.  We have spent quite some effort on
explaining these techniques to a security audience that is unfamiliar
with them, though we can of course always do better (particularly in
Section 4, as pointed out by review 19C, and in some other places).
However, reviewer 19C's comment is very accurate: "The authors have a
very large amount of material to summarise in a relatively small
amount of space, and the logical machinery is highly sophisticated and
builds on years of advanced work in this space. So this was always
going to be a non-trivial task."  We believe it is important to apply
state-of-the-art reasoning techniques from PL research to security
applications, so we hope CSF will not consistently assess these
techniques as being too complicated.


## Review 19A:

Reviewer A is concerned that the examples discussed in the paper are
fairly small: "(It doesn't help that the only examples discussed are
  assembly programs that fit in quarter-page figures. Limitations might
  become apparent connecting to, say, a fully abstract compiler for a
  typed source language.)"

It is true that our examples are small and synthetic, but they are
very challenging and have been chosen carefully to test whether the
semantic model is strong enough to allow one to prove expected
properties for them.  For what it's worth, we do in fact have some
experience with proofs of full abstraction for compilers and such a
result would be a very substantial additional result that could easily
fill a separate paper.  We see the current paper as a step towards
that goal that is independently useful.

  "One limitation whose importance I have a hard time gauging: local
  capabilities may only be kept in registers or stack.  We can't have
  arbitrary heap data structures full of different local capabilities.
  Is that an important limitation?  Do we ever want to do efficient
  lookup of local capabilities from semantically meaningful keys?"

You seem to be asking about storing local capabilities in data
structures like hashtables or trees which are typically allocated on
the heap.  In our setting, local capabilities cannot be stored on the
heap because there can be no store-local pointers for it.  However,
it's perfectly valid to allocate data structures containing local
capabilities on the stack.  They will of course be temporary, just
like the local capabilities they contain, but we believe that's
reasonable.

  ""For simplicity, we assume that memory allocated through malloc
  cannot be freed." Is this a show-stopper assumption to scale up to
  realistic code?"

We do believe reuse of malloc-allocated memory is important in real
code.  However, an explicit free call is hard to support in
capability-machine because use-after-frees may allow an adversary to
access memory that has been recycled and now contains capabilities of
the trusted code.  Supporting a free would require a notion of
capability revocation, which is hard to support efficiently.  A better
solution for reuse of malloc-allocated memory would be the use of a
trusted garbage collector which can guarantee the absence of
use-after-frees and which we believe would combine well with our
approach.

  "... And is it essential that malloc is a built-in instruction
  rather than a library function?"

Malloc is not really a built-in instruction.  In the examples, what
appears to be a malloc instruction is actually a macro that invokes a
trusted malloc function implemented in software.  It's trusted in the
sense that there is a specification that we assume it will satisfy.
The specification is in the technical report, and specifies a very
standard contract for malloc.

  "How does all this adapt to the multicore setting?  Does a
  zero-out-range instruction appear to run atomically?  What about
  context switches in the midst of other standard code snippets you
  insert?"

Generally, the stack is (as always) assumed to be thread-local. Since
we only apply the zero-out instruction (or its equivalent loop-based
implementation) to stack memory, we don't think this will be an issue,
even if we extend the approach to a concurrent setting (which is
currently out of scope).

  "At one point in the paper, there is a "throw-away" remark that
  stack-smashing attacks are straightforward to block.  I don't see
  how, if each function receives a pointer conveying access to its
  full stack area.  Such pointers must be carefully restricted into
  subpointers for particular local variables, like arrays.  Wouldn't
  it be easy to forget to restrict or to do it incorrectly?  I suppose
  the main arguments of this paper are about protecting *well-written*
  programs from attackers, but then don't well-written programs just
  avoid buffer overflows statically?"

Well, this was indeed poorly explained but the idea is that a compiler
for a high-level language might be able to guarantee that the code it
generates itself is well-written, but not that the programmer's code
is well-written too.  In such a setting, stack-smashing could be
prevented by the compiler code even in the presence of
not-well-written programmer code, by only giving the programmer access
to appropriately-bounded stack capabilities.

  "I'm stuck on type-level details of the future-worlds relations in
  Section 4.1.1.  Temporary and permanent regions are expressed as
  4-tuples, but the inference rules for future worlds operate on
  5-tuples.  Why is that?"

In Theorem 1, the set Rels consists of pairs of relations which is why
what appears to be a 4-tuple is really a 5-tuple.  We apologise for
the confusion.

  "Can you give an intuition for why the logical relations should be
  step-indexed?  The restricted nature of local capabilities seems to
  suggest that they won't be involved in the kinds of self-reference
  that motivated the invention of step indexing.  Is it the global
  references that force step indexing?"

No, local capabilities also require step-indexing.  An intuitive way
to see this is that it's easily possible to construct cycles in
memory, both through global capabilities and non-write-local memory,
but also just as easily through local capabilities and write-local
memory.  Once you have a memory with cycles, you need a way to break
loops to prevent cyclic reasoning and that loop-breaking is provided
by step-indexing.

  "... In general, can you give an intuition for why simpler proof
  techniques wouldn't suffice?  (It's hard not to suspect a bit of
  overkill when the only example programs are as short as they are.)"

This question is hard to answer since it's not really clear what
"simpler proof techniques" you have in mind.  However, the system that
we are trying to reason about is non-trivial: it contains
encapsulation primitives, mutable references and a higher-order heap
that may contain cycles.  For a lambda calculus with similar features,
the techniques we use are more or less standard and we are not aware
of simpler proof techniques that can derive results similar to the
ones we prove. Another way to understand the need for these methods is
as follows: The properties given by a capability depends on what the
invariants of the memory are.  The world contains the invariants, one
for each region. An invariant can refer to properties of other
capabilities.  Hence properties refer to invariants which refer to
properties - a cycle!

  "In Figure 9, is the beginning of `g1` right, where the 3rd line
  seems to overwrite the program counter, effectively jumping
  elsewhere without saving a new return pointer?  Should the operand
  order be reversed?"

Yes, thank you for pointing this out.

## Review 19B

We want to start by clarifying what we believe to be a
misunderstanding.  We do not see the capability machine formalisation
as the next capability machine.  Instead we see this work as general
work on developing proof techniques for capability machines that match
state-of-the-art proof techniques for high-level languages.  This is
also why we in the first contribution write that it is a "simple but
representative capability machine".  We hope and believe that the
proof techniques presented here can be used on real capability
machines, such as CHERI, as well.  Like-wise for the calling
convention, we believe that it can be used on real machines with the
necessary capabilities.
  
  "The second contribution in the Intro states that the capability
  machine "does not use separate per-component stacks but a single,
  fairly standard, contiguous stack", why is this a good idea?"

Well, the phrasing suggests otherwise, but the fact that it uses a
single stack was not really intended as a quality in itself.  It does
mean that we stay relatively close to established practice in
compilers in this respect, which may be considered an advantage
perhaps.

  "In any case, the paper would benefit from a high-level overview of
  the capability-based runtime architecture. Will there be any
  operating system running on the machine? How many different types of
  runtime components do we see on the machine? What are
  cross-component calls like? Are these supposed to run at the user
  level or within the OS kernel? Is there concurrency? How will kernel
  threads or user processes fit into the overall picture?"

Essentially we assume a single-threaded application running in a
private address space.  The components all live in this address space
and may communicate over higher-order interfaces.  Cross-component
calls are scalls and returns.  Whether there is a kernel outside of
this private address space (as in CHERI's hybrid architecture) or
whether the current application is the only code running on the system
does not really matter for our results.

  "What benefits will capabilities bring over the existing
  non-capability-based hardware?"

In a capability-machine, a component is a much more fine-grained
notion than, for example, a process in a standard OS.  You may imagine
an OO application where every object is a component, and its private
state is protected from interference by every other object, and our
calling convention is used to guarantee well-bracketed control flow.
A similarly fine-grained isolation seems impossible to attain using
non-capability-based hardware.

  "One way to control the complexity of [call,scall,push,pop macros]
  is to probably introduce a more abstract notion of "stacks" and
  treat these function call/return macros as actual built-in
  instructions in Figure 3. This way, we can see clearly how function
  calls/returns would work under various settings described in Section
  3."

That would work, but that amounts to considering a system with a
trusted stack manager. The point of our paper is that this is not
needed and we can implement an efficient calling convention with the
same effect in terms of a more lower-level primitive, namely local
capabilities. In future work, we are considering an approach such as
what you suggest and show a formal correspondence of such a system
with our current one.

## Review 19C

  "In general, the proposed approach is couched in terms of defending
  against an adversary (who is bad), who might be trying to attack
  "us" (who are necessarily good). In practice, capability systems are
  for programming in the presence of mutual distrust. In this case,
  the adversary wants to know they have enough stack space, might want
  to provide their own etc., and so one can't help but feel that the
  presented model is limited."

This is a misunderstanding: our model is not limited in this way, as
we discuss this in the discussion section under "Modularity": "It is
important that our calling convention is modular, i.e. we do not
assume that our code is specially privileged w.r.t. the adversary, and
they can apply the same measures to protect themselves from us as we
do to protect ourselves from them."

  "As an aside, to the above, the system wide limitation in the
  calling convention that local capabilities be used only for the
  single stack seems suboptimal."

The limitation is actually less severe than you describe: it's
perfectly fine to pass a local read-write capability to an adversary
(or vice versa) and our calling convention allows this.  However, we
cannot allow the same for callbacks, i.e. *executable* capabilities
that are not the return pointer.  There is also the system-wide
restriction that every write-local capability must itself be local,
but that also does not preclude using local capabilities for other
reasons than the stack.

  "Why not adopt a model in which local capabilities also denote a
  "region" or "domain" to which they belong, of which there can be an
  arbitrary number, and stipulate that the adversary must have local
  capabilities only to their own distinct domain. One expects that a
  more general calling convention would emerge (necessarily
  complicating the logical relation and its existing notion of
  regions) that would e.g. allow the adversary to have their own stack
  if they wished (desirable if it wishes to ensure it won't run out of
  stack space) and allowing a more expressive style of programming."

There are several problems with the model you describe: where would
the adversary store its return pointer (a local capability coming from
someone else's "domain") and where would we (the trusted code) store
ours? Additionally, it's far from clear how your proposed system would
scale to higher-order settings, i.e. how do callbacks get access to
their (local) stack pointer when they are invoked?

  "I can't say I was convinced by the footnote claiming that an
  adversary wanting to give temporary access to a callback that
  doesn't live on the single stack is not essential. What evidence is
  there for this claim?":

That's indeed a matter of perspective, as it depends on what you want
to enforce.  What we mean with this footnote is that it's not a goal
of the current work.  We will rephrase the footnote.

===========================================================================
                                  Comment
---------------------------------------------------------------------------
Following the PC discussion, ultimate decision was to reject this paper. Main factors were:

1. the density of technical presentation and formalism, with shepherding not possible here
2. some concern as to whether, despite the author response, the complexity of the formal machinery really is needed: whether a simpler memory model would in fact suffice

